---
title: [AWS SAA chapter 10. Amazon S3]
author: excelsiorKim
date: 2024-04-14 12:26:03 +0900
categories: [Cloud, AWS, SAA]
tags: [Cloud, AWS, SAA, S3]
keywords: [Cloud, AWS, SAA, S3]
description: SAA 자격증 공부 기록
toc: true
toc_sticky: true
---

### Amazon S3 Use cases

- 백업과 스토리지로 활용된다.
  - 파일용일 수도 있고, 디스크 용일 수도 있다.
- 재해 복구의 용도로도 쓰인다.
- Archive 용도로 사용된다.
- Hybrid Cloud Storage
- Application Hosting
- Media Hosting
- Data lakes & Big Data Analytics
- Static Website

### Amazon S3 - Buckets

- Amazon S3는 파일을 버킷에 저장하는데, 버킷은 상위 레벨 디렉토리로 표시된다.
- S3 버킷의 파일은 객체라고 하며, 이 버킷은 계정 안에 생성된다.
- 버킷에는 전역적으로 고유한 이름이 있어야 한다.
  - 즉, 이름은 계정에 있는 모든 리전과 AWS에 존재하는 모든 계정에서 고유해야한다.
- 버킷은 region level에서 정의된다.
- S3는 전역 서비스처럼 보이지만 버킷은 사실상 리전에서 생성된다.
- 명명규칙이 존재한다.
  - 버킷 이름에는 대문자나 밑줄이 없어야 한다.
  - 길이는 3자에서 63자 사이여야 한다.
  - IP여서는 안된다.
  - 소문자나 숫자로 시작해야 된다.
  - 문자와 숫자, 하이픈만 사용된다

### Amazon S3 - Objects

- 객체나 파일에는 키라는 것이 존재한다.
- Amazon S3 키는 파일의 전체 경로이다.
  - ex : s3://my-bucket/my_file.txt
- 즉, 키는 Prefix + object-name으로 구성되어 있다.
- Amazon S3 그 자체로는 디렉토리의 개념은 없다.
  - 키로 이뤄진 스토리지 이다.
- 키의 값은 본문의 내용이다.
- 파일 등 원하는 것은 뭐든지 Amazon S3로 업로드 할 수 있다.
  - 객체의 최대 크기는 5TB이다.
  - 업로드할 파일의 크기가 5GB이상이면 멀티파트 업로드를 사용해서 파일을 여러 부분으로 나눠 업로드해야 한다.
- 객체에는 메타데이터도 존재한다. (객체의 키 - 값 pair list를 말한다.)
  - 이는 시스템이나 사용자에 의해 설정되어 파일에 관한 요소나 메타데이터를 나타낼 수 있다.
- 태그는 최대 10개의 키-값 쌍을 가질 수 있다.
  - 보안과 수명 주기에 유용하다. 혹은 객체의 버전 ID를 갖기도 한다.

### Amazon S3 - Security

- User-Based
  - IAM Policies : 어떤 API 호출이 특정 IAM 사용자를 위해 허용되어야 하는지를 승인한다.
- Resource-Based
  - Bucket Policies : S3 콘솔에서 직접 할당할 수 있는 전체 버킷 규칙이다.
    - 특정 사용자가 들어올 수 있게 하거나 다른 계정의 사용자를 허용할 수 있다.
    - 이를 S3 버킷에 액세스할 수 있는 교차 계정이라고 한다.
  - Object Access Control List(ACL)
  - Bucket Access Control List(ACL)
- IAM Principal이 S3 객체에 액세스 할 수 있는 상황
  - IAM 권한이 이를 허용하거나 리소스 정책이 이를 허용하는 경우
  - 또는 명백한 거부가 없을 때
- Encryption : 암호 키를 사용하여 객체를 암호화 하는 것

### S3 Bucket Policies

![S3-JSON-img](/assets/img/2024-04-14-S3/S3-JSON.png)

- JSON based policies
  - Resource : buckets and objects
    - 이 정책에 적용되는 버킷과 객체를 정책에 알려줌
  - Effect : Allow / Deny
  - Actions : Set of API to Allow or Deny
    - 허용하거나 거부할 수 있는 API 집합
  - Principle : the account or user to apply the policy
- Use S3 bucket for policy to
  - grant public access to the bucket
  - Force objects to be encrypted at upload
  - Grant access to another account(Cross Account)

### Bucket Settings for Block Public Access

![Bucket-Settings-for-Block-Public-Access-img](/assets/img/2024-04-14-S3/Bucket-Setting-for-Block-Public-Access.png)

- 이 설정은 기업 데이터 유출을 방지하기 위한 추가 보안 계층으로서 AWS가 개발했다.
- S3 버킷 정책을 설정하여 공개로 만들더라도 이 설정이 활성화되어 있다면 버킷은 결코 공개되지 않는다.

### Amazon S3 - Versioning

![S3-Versioning-img](/assets/img/2024-04-14-S3/S3-versioning.png)

- S3애서 버전을 관리할 수 있다.
- bucket level에서 활성/비활성화를 할 수 있다
- 동일한 키를 업로드하고 해당 파일을 덮어쓰는 경우 버전 등을 생성하게 된다.
- 의도치 않은 삭제에 대해 보호해준다.
- 단, 버전 관리를 활성하기 전에 버전 관리가 적용되지 않은 모든 파일은 NULL 버전을 갖게된다. 또한 버전관리를 중단해도 이전 버전을 삭제하지는 않는다.

### Amazon S3 - Replication(CRR & SRR)

- S3 버킷을 복제하기 위해서는 소스 버킷과 복제 대상 버킷 둘 모두 버전 관리 기능이 활성화되어야 한다.
- CRR(Cross-Region Replication) : 두 리전이 달라할 때
- SRR(Same-Region Replication) : 같은 리전일 때
- 버킷은 서로 다른 AWS 계정 간에도 사용할 수 있다.
- 복제는 비동기식으로 이루어진다.
- 복제 기능이 정상적으로 실행되려면 S3에 올바른 IAM 권한(읽기, 쓰기 권한)을 S3에 부여해야 된다.
- Use case

  - CRR - 법규나 내부 체제 관리, 그리고 데이터가 다른 리전에 있어 발생할 수 있는 지연 시간을 줄일 경우에 사용한다.
  - SRR - 다수의 S3 버킷 간의 로그를 통합할 때나, 개발 환경이 별로도 있어 운영 환경과 개발 환경간의 실시간 복제를 필요로 할 때 사용될 수 있다.

- 복제를 활성화한 후에는 새로운 객체만 복제 대상이 된다.
- 기존의 객체를 복제하려면 S3 Batch Replication 기능을 사용해야 한다.
  - 기존 객체부터 복제에 실패한 객체까지 복제할 수 있는 기능이다.
- 삭제 작업에서는
  - 소스 버킷에서 대상 버킷으로 삭제 마커를 복제하면 된다.(설정에서 선택할 수 있다)
  - 버전 ID로 삭제하는 경우 버전 ID는 복제되지 않는다.(영구 삭제 기능임)
    - 악의적인 목적으로 영구 삭제하는 것을 방지하기 위함
- 체이닝 복제는 불가능하다.
  - 1번 버킷을 복제한 2번 버킷이 있을때, 2번 버킷을 복제한 3번 버킷은 1번 버킷의 내용을 복제하지 않는다.

### S3 Storage Classes

- Amazon S3 Standard - General Purpose
- Amazon S3 Standard - Infrequent Access(IA)
- Amazon S3 One Zone - Infrequent Access
- Amazon S3 Glacier Instant Retrieval
- Amazon S3 Glacier Flexible Retrieval
- Amazon S3 Glacier Deep Archive
- Amazon S3 Intelligent Tiering

- Amazon S3에서 객체를 생성할 때 클래스를 선택할 수도 있고 스토지리 클래스를 수동으로 수정할 수도 있다.
- 혹은 Amazon S3 수명 주기 구성을 사용해 스토리지 클래스 간에 객체를 자동으로 이동시킬 수 있다.

### S3 Durability and Availability

- Durability
  - 객체가 손실되는 횟수를 나타냄.
  - 일레븐 나인 이라는 내구성을 가진다.(10000년에 한번 객체 손실)
  - 모든 스토리지 클래스의 내구성은 동일하다.
- Availability
  - 서비스가 얼마나 용이하게 제공되는지를 나타낸다.
  - 스토리지 클래스에 따라 다르다.
  - 예를 들어 S3 standard의 경우 99.99%의 가용성을 갖는다. 이는 1년에 53분 동안은 서비스를 이용할 수 없다는 의미이다.
    - 따라서 애플리케이션을 개발할 때 이부분을 고려해 두어야 한다.

### S3 Standard - General Purpose

- 기본적으로 사용하는 스토리지 유형이다.
- 99.99%의 가용성을 갖는다
- 자주 사용하는 데이터를 보관하는 목적이다
- 지연 시간이 짧고 처리량이 높으며 AWS에서 두 개의 기능 장애를 동시에 버틸 수 있다.
- Use Case : 빅데이터 분석, 모바일과 게임 애플리케이션, 콘텐츠 배포 등이 있다.

### S3 Storage Classes - Infrequent Access

- 자주 엑세스 하지는 않지만 필요한 경우 빠르게 액세스 해야하는 데이터를 보관한다.
- S3 Standard보다 적은 비용이 든다.
  - 하지만 검색 비용이 발생한다.
- **Amazon S3 Standard-Infrequent Access(S3 Standard-IA)**
  - S3 Standard-IA의 가용성은 99.9% 이다(Standard 보다 약간 떨어짐)
  - 사용 사례 : 재해 복구와 백업
- **Amazon S3 One Zone-Infrequent Access(S3 One Zone-IA)**
  - 단일 AZ 내에서는 높은 내구성을 갖지만 AZ가 파괴된 경우 데이터를 잃게 된다.
  - 가용성은 더 낮은 수준인 99.5%이다.
  - 사용 사례 : 온프레미스 데이터를 2차 백업하거나 재생성 가능한 데이터를 저장하는 데 쓰인다.

### Amazon S3 Glacier Storage Classes

- 콜드 스토리지이다.
- 아카이빙과 백업을 위한 저비용 객체 스토리지 이다.
- 비용에는 스토리지 비용과 검색 비용이 들어간다.
- **Amazon S3 Glacier Instant Retrieval**
  - 밀리초 단위로 검색이 가능하다.
  - 예를 들어 분기에 한번 씩 액세스 하는 데이터에 적합하다.
  - 최소 보관 기간이 90일 이기 때문에 백업이지만 밀리초 이내에 액세스해야 하는 경우 적합하다.
- **Amazon S3 Glacier Flexible Retrieval**
  - 세가지 옵션이 존재함
    - Expedited : 1분 ~ 5분 이내에 데이터를 받을 수 있다.
    - Standard 3시간 ~ 5시간 이내에 데이터를 받을 수 있다.
    - Bulk : 5시간 ~ 12시간 이내에 데이터를 받을 수 있다.(무료이다)
  - 최소 보관 기간은 90일이다.
- **Amazon S3 Glacier Deep Archive**
  - 장기간 보관을 위한 스토리지 이다.
  - 두가지 검색 티어가 있다
    - Standard : 12시간
    - Bulk : 48시간
  - 최소 보관 시간이 180일이다.

### S3 Intelligent-Tiering

- 사용 패턴에 따라 액세스된 티어 간에 객체를 이동할 수 있게 해준다.
- 소액의 월별 모니터링 비용과 티어링 비용이 발생한다.
- 하지만 검색 비용이 들지 않는다.
- Frequent Access Tier (Automatic): default tier
- Infrequent Access Tier (Automatic) : 30일 동안 액세스하지 않는 객체 전용 티어
- Archive Instant Access 티어(Automatic) : 90일 동안 액세스하지 않는 티어 전용이다.
- Archive Access tier(Optional) : 90일에서 700일 이상까지 구성 가능
- Deep Archive Access Tier(Optional) : 180일에서 700일 이상 액세스하지 않는 객체에 구성할 수 있다.

### Amazon S3 - Moving Between Storage Classes

![Storage-Hierachy-img](/assets/img/2024-04-14-S3/Storage-Hierachy.png)

- 자주 접근되지 않는 object의 경우 **Standard-IA**로 옮길 수 있다.
- 빠른 접근이 필요하지 않는 아카이브 객체의 경우 **Glacier 또는 Glacier-Deep-Archive**로 옮길 수 있다.
- 이런 객체들을 수작업으로 올길 수도 있고 라이프 사이클 규칙을 이용해서 자동으로 옮길 수도 있다.

### Amazon S3 - Lifecycle Rules

- Transition Actions : 다른 스토리지 클래스로 이전하기 위해 객체를 설정하는 것
  - 에를 들어 : 생성된지 60일 후에는 Standard-IA, 6개월 후에는 Glacier
- Expiration Actions : 일정 시간 뒤에 만료되어 객체를 삭제하도록 설정하는 것
  - 에를 들어 액세스 로그 파일의 경우 365일 후에 삭제할 수 있게 설정할 수 있다.
  - 또는 파일의 버전들을 삭제할 수 있다.
  - 그리고 불완전한 멀티파트 업로드를 삭제할 수 있다.
- 특정 접두어에 대한 규칙
  - 버킷 전체에 적용하거나 버킷 안의 특정한 경로에 적용할 수 있다.
  - 또는 특정 객체 태그에 대해 규칙을 지정할 수도 있다.

### Scenario 1

- 상황 : EC2 애플리케이션이 있고, 그 앱은 프로필 사진이 Amazon S3에 업로드 된 후에 이미지 섬네일을 생성한다. 하지만 섬네일들은 원본 사진으로부터 쉽게 재생성할 수 있다. 그리고 60일 동안만 보관해야 된다. 하지만 원본 이미지는 60일 동안 곧바로 받을 수 있어야 한다.(그 후에 사용자는 최대 6시간 동안 기다릴 수 있다)

- 해결책
  - S3 원본 이미지는 Standard 클래스에 있을 수 있고, 60일 후에 Glacier로 이전하기 위한 라이프 사이클 설정을 한다.
  - 섬네일의 경우 One-Zone IA에 관리하고, 라이프 사이클 설정을 60윌 뒤에 삭제되도록 설정한다.

### Scenario 2

- 상황 : 회사에서 규칙에 따라 30일 동안은 삭제된 S3 객체를 즉각적으로 복구할 수 있어야 한다. 그 기간이 지나면 최장 365일 동안은 삭제된 객체를 48시간 이내에 복구할 수 있어야 한다.
- 해결책
  - S3 Versioning을 활성화해서 객체 버전을 보관할 수 있다.
  - 삭제된 객체들은 실제로 삭제마커에 의해 감춰져 있기 때문에 복구할 수 있게 된다.
  - 최상위 버전(현재 버전)이 아닌 객체들을 Standard-IA로 이전하기 위한 규칙을 만든다. 즉 현재 버전이 아닌 버전들을 아카이브화 목적으로 Glacier Deep Archive로 이전할 수 있다.

### Amazon S3 Analytics - Storage Class Analysis

![S3-Analytics-img](/assets/img/2024-04-14-S3/S3-Analytics.png)

- 객체를 다른 클래스로 이전할 최적의 일 수를 결정하기 위해서는 Amazon S3 Analytics를 이용하면 된다.
- Standard 와 Standard-IA에 관한 추천 사항을 제시해준다
  - One-Zone IA 또는 Glacier와는 호환되지 않는다.
- 보고서가 매일 업데이토 되고, 데이터 분석 결과가 나오는게 보일 때까지 24시간 내지 48시간이 걸린다.

### S3 - Requester Pays

- 버킷 소유자가 버킷과 관련된 모든 Amazon S3 스토리지 및 데이터 전송 비용을 지불한다.
- 그러나 수 맣은 대형 파일이 있고 일부 고객이 이를 다운로드하려고 하면 요청자 지불 커빗을 활성화 해야 된다.
- 이 경우 버킷 소유자가 아니라 요청자가 객체 데이터 다운로드 비용을 지불한다.
- 이를 통해 요청자가 객체를 다운로드 요청하면 그 요청자가 다운로드와 관련된 네트워킹 비용을 지불하게 된다.
- 대량의 데이터 셋을 다른 계정과 공유하려고 할 때 유용하다.
  - 그러기 위해선 요청자가 익명이어서는 안된다
  - 요청자가 AWS에서 인증을 받아야 한다.

### S3 Event Notification

![S3-Event-img](/assets/img/2024-04-14-S3/S3-Event.png)

- 이벤트 : 객체의 생성, 삭제, 복구, 복제되는 것을 말한다.
- 사용자는 그런 이벤트들을 필터링 할 수 있다.
- SNS 토픽, SQS Queue, Lambda가 대상이 될 수 있다.
- 이벤트는 통상적으로 몇초에서 몇분 사이로 전달된다.

### S3 Event Notification - IAM Permission

![S3-Event-Notification-img](/assets/img/2024-04-14-S3/S3-IAM-Permission.png)

- 이벤트 알림이 작동하려면 아래와 같이 해당 서비스가 S3에 대한 IAM 권한을 갖고 있어야 한다.
- S3에서 정책을 정의하는 것이 아닌 SNS 토픽, SQS Queue, Lambda에서 리소스 액세스 정책을 정의한다.

### S3 Event Notification with Amazon EventBridge

- 모든 이벤트는 결국 Amazon EventBridge로 간다.
- EventBridge에서 규칙을 설정할 수 있고, 그 규칙들 덕분에 이벤트들을 18가지의 AWS 서비스에 전송할 수 있다.
  - 추후에 게시물로 다룰 예정

### S3-Baseline Performance

- 기본적으로 Amazon S3는 요청이 아주 많을 때 자동으로 확장된다.
- S3는 버킷 내에서 접두사당 초당 3500개의 PUT/COPY/POST/DELETE와 초당 5500개의 GET/HEAD 요청을 지원한다.
- 버캣 내에서는 접두사(Prefix) 수에 제한이 없다.
  - 여기서 말하는 접두사란 bucket과 file 사이에 있는 것을 말한다
  - 예를 들어 bucket/folder1/sub1/file 이라면 /folder1/sub1 이 Prefix이다
- 즉 네 개의 접두사가 있는 경우 읽기 요청을 균등하게 분산하면 5500 \* 4 = 22000의 읽기 성능을 보여줄 수 있다.

### S3-Performance

![S3-Performance-img](/assets/img/2024-04-14-S3/S3-Performance.png)

- Multi-Part upload
  - 100MB가 넘는 파일은 멀티파트 업로드를 사용하는 것이 좋다
  - 5GB가 넘는 파일은 반드시 사용해야 한다.
  - 멀티파트 업로드는 업로드를 병렬화하므로, 전송 속도를 높여 대역폭을 최대화 할 수 있다.
- S3 Transfer Acceleration
  - 파일을 AWS 엣지 로케이션으로 전송해서 전송 속도를 높이고 데이터를 대상 리전에 있는 S3 버킷으로 전달한다
    - 엣지 로케이션은 리전보다 수가 많다
  - 멀티파트 업로드와 같이 사용할 수 있다
  - 퍼블릭 인터넷의 사용량을 최소화하고 프라이빗 AWS 네트워크의 사용량을 최대화하는 방식이다.

### S3 Performance - S3 Byte-Range Fetches

![Byte-Range-Fetch-img](/assets/img/2024-04-14-S3/Byte-Range-Fetch.png)

- 파일에서 특정 바이트 범위를 가져와서 GET 요청을 병렬화 한다.
- 특정 바이트 범위를 가져오는데 실패한 경우에도 더 작은 바이트 범위에서 재시도하므로 실패의 경우에도 복원력이 높다.
- 사용사례 1 : 병렬화를 통해 다운로드 속도를 높일 때이다.
- 사용사례 2 : 파일의 일부만 검색하는 경우

### S3 Select와 Glacier Select

![S3-Filter-img](/assets/img/2024-04-14-S3/S3-Filter.png)

- S3에서 파일을 검색할 때 검색한 다음에 필터링하려면 너무 많은 데이터를 검색하게 된다.
- 이를 방지하기 위해 SQL문을 통해 간단히 server-side filter을 해줄 수 있다.
- 네트워크 전송이 줄어들기 때문에 데이터 검색과 필터링에 드는 클라이언트 측의 CPU 비용도 줄어든다.
- 간단한 필터링에 적합하다

### S3 Batch Operation

![S3-Batch-Operations-img](/assets/img/2024-04-14-S3/S3-Batch-Operations.png)

- 단일 요청으로 기존 S3 객체에서 대량 작업을 수행하는 서비스이다.
  - 한 번에 많은 S3 객체의 메타데이터와 프로퍼티를 수정할 수 있다.
  - 배치 작업으로 S3 버킷 간에 객체를 복사할 수 있다.
  - **S3 버킷 내의 암호화되지 않는 모든 객체를 암호화 할 수 있다.**
  - ACL이나 태그를 수정할 수 있다.
  - S3 Glacier에서 한 번에 많은 객체를 복원할 수 있다.
  - Lambda 함수를 호출해 S3 Batch Operations의 모든 객체에서 사용자 지정 작업을 수행할 수도 있다.
  - 객체 목록에서 원하는 작업은 무엇이든지 수행할 수 있다.
- 작업은 객체의 목록, 수행할 작업 옵션 매개 변수로 구성된다.
- S3 Batch Operations를 사용하면
  - 재시도를 관리할 수 있다
  - 진행 상황을 추적할 수 있고 작업 완료 알림을 보낸다.
  - 보고서 생성 등을 할 수 있다.
- S3 Inventory라는 기능을 사용해 객체 목록을 가져오고, S3 Select를 사용해 객체를 필터링한다.
